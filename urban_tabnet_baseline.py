# -*- coding: utf-8 -*-
"""urban tabent and xgboost.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oCJIKijjMF3UjKyVxJ6yX_nYMRlXIKtM
"""

n_targets = 8

## !pip install pytorch_tabnet
## !pip install wget

"""# Customize a TabNet Model

## This tutorial gives examples on how to easily customize a TabNet Model

### 1 - Customizing your learning rate scheduler

Almost all classical pytroch schedulers are now easy to integrate with pytorch-tabnet

### 2 - Use your own loss function

It's really easy to use any pytorch loss function with TabNet, we'll walk you through that


### 3 - Customizing your evaluation metric and evaluations sets

Like XGBoost, you can easily monitor different metrics on different evaluation sets with pytorch-tabnet
"""

# Commented out IPython magic to ensure Python compatibility.
from pytorch_tabnet.tab_model import TabNetClassifier
from pytorch_tabnet.tab_model import TabNetRegressor

import torch
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import roc_auc_score

import pandas as pd
import numpy as np
np.random.seed(0)


import os
import wget
from pathlib import Path

from matplotlib import pyplot as plt
# %matplotlib inline

#from google.colab import drive
#drive.mount('/content/drive')

#!cp /content/drive/MyDrive/fixed_data.zip .
#!cp /content/drive/MyDrive/sensor-data-clean.tar.gz .

#!unzip -q fixed_data.zip
#!tar -xvf sensor-data-clean.tar.gz

dataset_folders = ['Sensor01', 'Sensor02',
                   'Sensor03', 'Sensor04',
                   'Sensor05', 'Sensor06',
                   'Sensor07', 'Sensor08',
                   'Sensor09', 'Sensor10',
                   ]

train = pd.read_csv("sensor-data-clean.csv", index_col=0)

print(train)

train = train.dropna()
train = train.reset_index(drop=True)

if "Set" not in train.columns:
    train["Set"] = np.random.choice(["train", "valid", "test"], p =[.8, .1, .1], size=(train.shape[0],))

train_indices = train[train.Set=="train"].index
valid_indices = train[train.Set=="valid"].index
test_indices = train[train.Set=="test"].index

print(train_indices)

print(train)

"""### Simple preprocessing

Label encode categorical features and fill empty cells.
"""

wind_direction_dict = {
    'N': [0, 1], 'S': [0, -1], 'W': [-1,0], 'E': [1, 0],
    'NW': [-1, 1], 'NE': [1, 1], 'SW': [-1,-1], 'SE': [1, -1],
    'NNW': [-0.5, 1], 'NNE': [0.5, 1], 'SSW': [-0.5, -1], 'SSE': [0.5, -1],
    'WNW': [-1, 0.5], 'ENE': [1, 0.5], 'WSW': [-1, -0.5], 'ESE': [1, -0.5],
    'X': [0, 0], 'nan': [0, 0]
  }



"""## Preprocess wind direction

"""

wind_direction_x = []
wind_direction_y = []
count = 0
for i in train['Direction']:
  count = count +1
  try:
    wind_direction_x.append(wind_direction_dict[i][0])
    wind_direction_y.append(wind_direction_dict[i][1])
  except:
    print(file_name[count-1])
    print(train.iloc[count-1])

"""## Preprocess latlng"""

lat = []
lng = []
for i in train['Latitude']:
  lat.append(round(i, 4))
for i in train['Longtitude']:
  lng.append(round(i, 4))

"""## Preprocess time

"""

h = [] 
m = [] 

for i in train['Time']:
  count = count +1
  z = i.split(' ')[1]
  th, tm, ts = z.split(':')
  h.append(int(th))

print(len(h))

"""## Preprocess date

"""

import datetime

d = [] 
mon = [] 
wd = []
for i in train['Date']:
  count = count +1
  ty, tmon, td = i.split('-')
  d.append(int(td))
  mon.append(int(tmon))
  wd.append(datetime.datetime(int(ty),int(tmon),int(td)).weekday())

data = {
    'weekday': wd,
    'day': d,
    'month': mon,
    'hour': h,
    'lat': lat,
    'lng': lng,
    'alt': train['Altitude'],
    'temp': train['Temperature'],
    'humidity': train['Humidity'],
    'rain': train['Rainfall'],
    'wind_direc_x': wind_direction_x,
    'wind_direc_y': wind_direction_y,
    'wind_gust': train['WindGust'],
    'wind_avg': train['WindSpeed'],
    'pm1.0': train['PM1.0'],
    'pm2.5': train['PM2.5'],
    'pm10': train['PM10'],
    'uv': train['UV'],
    'co': train['CO'],
    'no2': train['NO2'],
    'so2': train['SO2'],
    'o3': train['O3'],
    'Set': train['Set']
}

full_target = ['pm1.0', 'pm2.5', 'pm10', 'co', 'no2', 'so2', 'o3']
target = ['pm1.0', 'pm2.5', 'pm10', 'co', 'no2', 'so2', 'o3']
unused_feat = ['Set']

final_train = pd.DataFrame(data)

final_train.dtypes

print(final_train)

"""### Define categorical features for categorical embeddings"""

categorical_columns = []
categorical_dims = []

features = [ col for col in final_train.columns if col not in unused_feat+target] 

cat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]

cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]

print(features)

# Network parameters
max_epochs = 100 if not os.getenv("CI", False) else 2
batch_size = 1024
clf = TabNetRegressor(
    n_d=4,
    n_a=4,
    cat_idxs=cat_idxs,
                       cat_dims=cat_dims,
                       cat_emb_dim=1,
                       optimizer_fn=torch.optim.Adam, # Any optimizer works here
                       optimizer_params=dict(lr=2e-2),
                       scheduler_fn=torch.optim.lr_scheduler.OneCycleLR,
                       scheduler_params={"is_batch_level":True,
                                         "max_lr":5e-2,
                                         "steps_per_epoch":int(train.shape[0] / batch_size)+1,
                                         "epochs":max_epochs
                                          },
                       mask_type='entmax', # "sparsemax",
                      )

"""### Training"""

X_train = final_train[features].values[train_indices]
y_train = final_train[target].values[train_indices]
#y_train = np.transpose(np.tile(y_train, (n_targets,1)))

X_valid = final_train[features].values[valid_indices]
y_valid = final_train[target].values[valid_indices]
#y_valid = np.transpose(np.tile(y_valid, (n_targets,1)))

X_test = final_train[features].values[test_indices]
y_test = final_train[target].values[test_indices]
#y_test = np.transpose(np.tile(y_test, (n_targets,1)))

clf.fit(
    X_train=X_train, y_train=y_train,
    eval_set=[(X_train, y_train), (X_valid, y_valid)],
    eval_name=['train', 'valid'],
    eval_metric=['mse'],
    max_epochs=max_epochs,
    patience=50,
    batch_size=20480, #virtual_batch_size=1024,
    num_workers=2,
    drop_last=False
)

from sklearn.metrics import mean_squared_error

preds = clf.predict(X_test)

test_mse = mean_squared_error(y_pred=preds, y_true=y_test)

print(f"BEST VALID SCORE FOR  : {clf.best_cost}")
print(f"FINAL TEST SCORE FOR  : {test_mse}")
